# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
#                                Default: apache/airflow:2.7.0
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''
#
# Feel free to modify this file to suit your needs.
---
# Set AIRFLOW_UID to match your WSL/host user ID to avoid permission issues
# Get your ID with: id -u
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  build:
    context: ..
    dockerfile: docker/Dockerfile
  image: assignment04:latest
  env_file:
    - ../.env
  environment:
    &airflow-common-env
    AIRFLOW_UID: "1000"
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__DAGS_FOLDER: /app/src/dags
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    # For backward compatibility, with Airflow <2.3
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    # Generate deterministic secret key for all components (required for multi-component sync)
    AIRFLOW__CORE__FERNET_KEY: 'gAAAAABlZzk0MWE4ODc2ZWVmZjkyYTk1ZDhkODNhOWM5NTk0MjExZjJhNmQ0ZjA4OTA1M2I3MzYxNzM0MWM4YWE4MWJmNThhODUxMQ=='
    AIRFLOW__WEBSERVER__SECRET_KEY: 'ThisIsASecretKeyForAirflow2025'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__MAXIMUM_PAGE_LIMIT: '1000'
    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'false'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    # Fix remote logging access (use local logging)
    AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
    AIRFLOW__LOGGING__LOG_LEVEL: 'INFO'
    # Hugging Face cache directory
    HF_HOME: /app/.cache/huggingface
    TRANSFORMERS_CACHE: /app/.cache/huggingface/transformers
    # DeepSearch GLM model cache
    DEEPSEARCH_GLM_CACHE_DIR: /app/.cache/deepsearch_glm
    # ChromeDriver configuration
    CHROMEDRIVER_PATH: /usr/bin/chromedriver
    CHROME_BIN: /usr/bin/chromium
    AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
    AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
    S3_BUCKET_NAME: ${S3_BUCKET_NAME}
  volumes:
    - ../src/dags:/app/src/dags
    - ../data:/app/data
    - airflow_logs:/app/logs
    - airflow_plugins:/app/plugins
    - ../.env:/app/.env
    - ../src:/app/src
    - ../config:/app/config
    - huggingface_cache:/app/.cache/huggingface
    - deepsearch_models:/app/.cache/deepsearch_glm
  user: "${AIRFLOW_UID:-1000}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

volumes:
  postgres-db-volume:
  airflow_logs:
  airflow_plugins:
  huggingface_cache:
  deepsearch_models:

services:
  postgres:
    image: postgres:13
    container_name: assignment04-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - assignment-network
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: on-failure

  airflow-webserver:
    <<: *airflow-common
    container_name: assignment04-airflow-webserver
    command: airflow webserver
    ports:
      - "8080:8080"
    networks:
      - assignment-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "-u", "admin:admin", "http://localhost:8080/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: on-failure
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: assignment04-airflow-scheduler
    hostname: airflow-scheduler
    command: airflow scheduler
    networks:
      - assignment-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: on-failure
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    container_name: assignment04-airflow-triggerer
    hostname: airflow-triggerer
    command: airflow triggerer
    networks:
      - assignment-network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: on-failure
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    container_name: assignment04-airflow-init
    entrypoint: /bin/bash
    user: "0:0"
    networks:
      - assignment-network
    command:
      - -c
      - |
        set -e
        echo "1. Checking Airflow version..."
        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO airflow version 2>/dev/null | head -n 1)
        echo "Detected Airflow version: $${airflow_version}"
        echo "✓ Airflow version OK"
        
        echo "2. Creating Airflow directories..."
        mkdir -p /app/logs /app/plugins /app/dags
        chown -R "$${AIRFLOW_UID:-1000}:0" /app/logs
        chown -R "$${AIRFLOW_UID:-1000}:0" /app/plugins
        chown -R "$${AIRFLOW_UID:-1000}:0" /app/dags
        echo "✓ Directories created"
        
        echo "3. Running Airflow database initialization..."
        airflow db migrate
        echo "✓ Database initialized"
        
        echo "4. Creating/updating admin user..."
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Airflow \
          --lastname Admin \
          --role Admin \
          --email admin@example.com 2>/dev/null || echo "User already exists"
        echo "✓ Admin user ready"
        
        echo "5. Airflow initialization complete"
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    volumes:
      - ../:/app/workspace

  # FastAPI Backend
  fastapi:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: assignment04-fastapi:latest
    container_name: assignment04-api
    command: bash -c "cd /app && uvicorn src.backend.rag_search_api:app --host 0.0.0.0 --port 8000 --reload"
    ports:
      - "8000:8000"
    networks:
      - assignment-network
    env_file:
      - ../.env
    environment:
      PYTHONUNBUFFERED: "1"
      ENVIRONMENT: docker
      PYTHONPATH: /app
    volumes:
      - ../src:/app/src
      - ../data:/app/data
      - ../.env:/app/.env
    working_dir: /app
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: on-failure

  # Streamlit Frontend
  streamlit:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: assignment04-streamlit:latest
    container_name: assignment04-ui
    command: bash -c "cd /app && streamlit run src/frontend/streamlit_app.py --server.port 8501 --server.address 0.0.0.0"
    ports:
      - "8501:8501"
    networks:
      - assignment-network
    env_file:
      - ../.env
    environment:
      PYTHONUNBUFFERED: "1"
      ENVIRONMENT: docker
      PYTHONPATH: /app
      STREAMLIT_SERVER_HEADLESS: "true"
      STREAMLIT_SERVER_PORT: "8501"
      STREAMLIT_SERVER_ADDRESS: "0.0.0.0"
      FASTAPI_HOST: "fastapi"
      FASTAPI_PORT: "8000"
    volumes:
      - ../src:/app/src
      - ../data:/app/data
      - ../.env:/app/.env
    working_dir: /app
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: on-failure

networks:
  assignment-network:
    driver: bridge
