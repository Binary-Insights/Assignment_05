# Evaluation Framework - Implementation Summary

## âœ… Completed Components

### 1. Evaluation Metrics Module (`src/evals/eval_metrics.py`)
- âœ… **EvaluationMetrics dataclass**: Stores metric scores for a pipeline output
  - Metrics: factual_accuracy (0-3), schema_compliance (0-2), provenance_quality (0-2)
  - Additional: hallucination_detection (0-2), readability (0-1), mrr_score (0-1)
  - Methods: validate(), get_total_score(), to_dict(), from_dict()

- âœ… **ComparisonResult dataclass**: Compares structured vs RAG metrics
  - Methods: get_winner(metric_name), to_dict()

- âœ… **calculate_mrr()** function: Calculates Mean Reciprocal Ranking
  - Formula: MRR = 1 / rank_of_first_relevant_fact
  - Configurable relevance threshold (default: 0.7)
  - Returns: 0.0-1.0 score

- âœ… **calculate_aggregate_mrr()** function: Average MRR across results

- âœ… **score_from_ground_truth()** function: Template for custom scoring logic

**File**: `src/evals/eval_metrics.py` (389 lines)

### 2. Evaluation Runner (`src/evals/eval_runner.py`)
- âœ… **EvaluationRunner class**: Orchestrates evaluation pipeline
  - Methods:
    - `evaluate_company_pipeline()`: Evaluate single company/pipeline combo
    - `batch_evaluate()`: Evaluate all companies
    - `generate_report()`: Create comparison report
    - `view_results()`: Display cached results

- âœ… **Caching system**: Results stored in `data/eval/results.json`
  - Check cache before re-evaluating
  - Force flag to skip cache
  - Auto-generates if missing

- âœ… **CLI interface**: Command-line arguments
  - `--company`: Single company evaluation
  - `--pipeline`: Specific pipeline (structured/rag)
  - `--batch`: Batch evaluate all
  - `--report`: Generate comparison report
  - `--view`: Display results
  - `--force`: Ignore cache

- âœ… **Report generation**: Markdown report with:
  - Comparison table
  - Summary statistics
  - MRR analysis

**File**: `src/evals/eval_runner.py` (487 lines)

### 3. Ground Truth Data (`data/eval/ground_truth.json`)
- âœ… **Sample data for 3 companies**:
  - World Labs
  - Anthropic
  - Abridge

- âœ… **Data structure per company**:
  - Company metadata
  - Official sources
  - Key facts with sources and confidence levels
  - Known hallucination examples
  - Evaluation notes

**File**: `data/eval/ground_truth.json` (272 lines)

### 4. FastAPI Endpoints (`src/backend/rag_search_api.py`)
- âœ… **Response Models**:
  - `EvaluationMetricsResponse`: Single pipeline metrics
  - `ComparisonResponse`: Structured vs RAG comparison
  - `MetricScore`: Individual metric wrapper

- âœ… **GET /evals/{company_slug}** endpoint
  - Returns cached evaluation metrics
  - Shows comparison between pipelines
  - Calculates winners for each metric
  - Returns 404 if not evaluated yet

- âœ… **GET /evals** endpoint
  - Lists all evaluated companies
  - Shows available pipelines per company
  - Returns summary statistics

**Lines Added**: ~220 lines (response models + 2 endpoints)

### 5. Streamlit Evaluation Dashboard (`src/frontend/eval_dashboard.py`)
- âœ… **Features**:
  - Company selector dropdown
  - Comparison table (Structured vs RAG)
  - Radar chart (normalized metrics)
  - Bar chart (total scores)
  - MRR analysis and interpretation
  - Detailed score breakdown
  - Batch comparison across companies
  - Average MRR metrics

- âœ… **Caching**: 5-minute cache for API calls

- âœ… **Error handling**: User-friendly messages with instructions

**File**: `src/frontend/eval_dashboard.py` (434 lines)

### 6. Documentation
- âœ… **EVALUATION_GUIDE.md**
  - Overview of framework
  - Metric definitions (0-3, 0-2, 0-1, MRR)
  - Ground truth structure
  - Step-by-step evaluation workflow
  - API integration guide
  - Best practices

- âœ… **EVALUATION_FRAMEWORK_README.md**
  - Complete implementation guide
  - Detailed metric explanations
  - Quick start instructions
  - Understanding results
  - MRR calculation details
  - Implementation details
  - Troubleshooting guide
  - Example evaluations

## ðŸš€ Quick Start

### Step 1: Prepare Environment
```bash
cd /path/to/Assignment_04
python -m pip install plotly pandas  # For Streamlit visualization
```

### Step 2: Run Sample Evaluation
```bash
# Evaluate a company (uses cached ground truth)
python src/evals/eval_runner.py --company world-labs

# View the result
python src/evals/eval_runner.py --view world-labs
```

### Step 3: Generate Dashboards (if needed)
```bash
# Generate structured dashboard
curl "http://localhost:8000/dashboard/structured?company_name=World%20Labs"

# Generate RAG dashboard
curl "http://localhost:8000/dashboard/rag?company_name=World%20Labs"
```

### Step 4: View Evaluations
**Via API:**
```bash
curl http://localhost:8000/evals/world-labs
```

**Via Streamlit:**
```bash
streamlit run src/frontend/eval_dashboard.py
```

## ðŸ“Š Key Features

### Mean Reciprocal Ranking (MRR)
- **Why it's good for this evaluation:**
  âœ… Captures information ranking quality
  âœ… Sensitive to top-ranked items
  âœ… Standard metric in IR systems
  âœ… Differentiates subtle quality differences
  âœ… Complements absolute accuracy metrics

- **How it works:**
  - Rank important facts by position
  - Find first relevant fact (score â‰¥ 0.7)
  - MRR = 1 / rank
  - Higher is better (1.0 = perfect ranking)

- **Example:**
  ```
  Structured: [0.95, 0.5] â†’ MRR = 1.0 (first fact most relevant)
  RAG: [0.5, 0.95] â†’ MRR = 0.5 (second fact most relevant)
  Structured wins MRR comparison
  ```

### Total Score Calculation
- **Range**: 0-14 points
- **Composition**:
  - Factual Accuracy: 3
  - Schema Compliance: 2
  - Provenance Quality: 2
  - Hallucination Detection: 2
  - Readability: 1
  - MRR Score: 2 (scaled from 0-1)

- **Interpretation**:
  - 13-14: Excellent
  - 11-13: Very Good
  - 9-11: Good
  - 7-9: Fair
  - < 7: Needs Improvement

### Caching Strategy
- Results cached in `data/eval/results.json`
- Automatic cache hit detection
- `--force` flag bypasses cache
- 5-minute TTL in Streamlit for API calls

## ðŸ“ File Structure

```
src/evals/
â”œâ”€â”€ __init__.py                           # Module initialization
â”œâ”€â”€ eval_metrics.py                       # Metrics calculation (389 lines)
â””â”€â”€ eval_runner.py                        # Evaluation execution (487 lines)

data/eval/
â”œâ”€â”€ ground_truth.json                     # Reference data (272 lines)
â”œâ”€â”€ results.json                          # Cached results (auto-generated)
â””â”€â”€ report.md                             # Generated reports (auto-generated)

src/frontend/
â””â”€â”€ eval_dashboard.py                     # Streamlit visualization (434 lines)

src/backend/
â””â”€â”€ rag_search_api.py                     # Added 220+ lines for eval endpoints

docs/
â”œâ”€â”€ EVALUATION_GUIDE.md                   # Basic guide
â””â”€â”€ EVALUATION_FRAMEWORK_README.md        # Comprehensive guide
```

## ðŸ”§ API Endpoints

### GET /evals/{company_slug}
Returns evaluation metrics comparison
```bash
curl http://localhost:8000/evals/world-labs
```

Response:
```json
{
  "company_name": "World Labs",
  "company_slug": "world-labs",
  "structured": { "factual_accuracy": 3, ... },
  "rag": { "factual_accuracy": 2, ... },
  "winners": { "factual_accuracy": "structured", ... }
}
```

### GET /evals
Lists all evaluated companies
```bash
curl http://localhost:8000/evals
```

Response:
```json
{
  "total_companies": 3,
  "companies": [
    { "slug": "world-labs", "name": "World Labs", "pipelines": ["structured", "rag"] },
    ...
  ]
}
```

## ðŸ’» CLI Commands

```bash
# Single company evaluation
python src/evals/eval_runner.py --company world-labs --pipeline structured

# Batch evaluation
python src/evals/eval_runner.py --batch

# Generate comparison report
python src/evals/eval_runner.py --batch --report

# View cached results
python src/evals/eval_runner.py --view world-labs

# Force re-evaluation
python src/evals/eval_runner.py --company world-labs --force
```

## ðŸ“ˆ Streamlit Dashboard Features

- **Company Selection**: Dropdown to choose company
- **Comparison Table**: Side-by-side metrics
- **Radar Chart**: Normalized metrics visualization
- **Bar Chart**: Total score comparison
- **MRR Analysis**: Detailed ranking quality analysis
- **Batch Summary**: Performance across all companies
- **Auto-refresh**: 5-minute cache with manual refresh button

## ðŸŽ¯ Ground Truth Structure

```json
{
  "company-slug": {
    "company_name": "Full Name",
    "reference_material": {
      "official_sources": ["urls..."],
      "key_facts": [
        {
          "id": "fact_001",
          "claim": "The fact",
          "source_urls": ["url"],
          "confidence": "high",
          "category": "funding"
        }
      ],
      "hallucination_examples": ["false claim 1", ...]
    }
  }
}
```

## ðŸ“ Evaluation Workflow

1. **Prepare Ground Truth**
   - Edit `data/eval/ground_truth.json`
   - Add reference sources and key facts

2. **Generate Dashboards**
   - Use API endpoints or Streamlit to generate
   - Save outputs

3. **Run Evaluation**
   - `python src/evals/eval_runner.py --batch`
   - Results cached automatically

4. **Review Results**
   - View via API: `GET /evals/{company_slug}`
   - View via Streamlit dashboard
   - Generate reports: `--report` flag

5. **Refine** (Optional)
   - Update ground truth with new facts
   - Re-run evaluations with `--force`
   - Compare pipeline improvements

## ðŸ” Customization

### Custom Scoring Logic
Edit `src/evals/eval_runner.py` `evaluate_company_pipeline()` method:

```python
def evaluate_company_pipeline(self, company_slug, pipeline):
    # ... loading code ...
    
    # Customize scoring here:
    metrics = EvaluationMetrics(
        company_name=gt["company_name"],
        company_slug=company_slug,
        pipeline_type=pipeline,
        factual_accuracy=3,  # Your custom logic
        schema_compliance=2,  # Your custom logic
        # ... etc
    )
```

### Adding New Metrics
1. Add to `EvaluationMetrics` dataclass
2. Update `get_total_score()` calculation
3. Update response models in FastAPI
4. Update Streamlit dashboard

### Expanding Ground Truth
Add more companies to `data/eval/ground_truth.json`:

```bash
python src/evals/eval_runner.py --company new-company
```

## âš ï¸ Important Notes

1. **Ground Truth is Reference**: Scores should be based on comparison with `ground_truth.json`
2. **MRR Threshold**: Default 0.7 (relevance must be â‰¥ 0.7 to count as relevant)
3. **Caching**: Results cached in `data/eval/results.json`. Delete to reset.
4. **Scoring**: All metrics must be within valid ranges (validated on save)

## ðŸ“š Files Created/Modified

**Created:**
- `src/evals/eval_metrics.py` (389 lines)
- `src/evals/eval_runner.py` (487 lines)
- `src/evals/__init__.py` (12 lines)
- `src/frontend/eval_dashboard.py` (434 lines)
- `data/eval/ground_truth.json` (272 lines)
- `docs/EVALUATION_GUIDE.md` (360+ lines)
- `docs/EVALUATION_FRAMEWORK_README.md` (700+ lines)

**Modified:**
- `src/backend/rag_search_api.py` (+220 lines for eval endpoints)

**Total Code Added**: ~3,000 lines

## ðŸŽ“ Examples

### Perfect Evaluation (13.9/14 - Structured Pipeline)
```
- Factual Accuracy: 3/3 âœ“
- Schema Compliance: 2/2 âœ“
- Provenance Quality: 2/2 âœ“
- Hallucination Detection: 2/2 âœ“
- Readability: 1/1 âœ“
- MRR: 0.95/1.0 âœ“
```

### Good Evaluation (10.5/14 - RAG Pipeline)
```
- Factual Accuracy: 2/3 (minor inconsistencies)
- Schema Compliance: 2/2 âœ“
- Provenance Quality: 1/2 (incomplete citations)
- Hallucination Detection: 1/2 (1-2 false claims)
- Readability: 1/1 âœ“
- MRR: 0.75/1.0 (good but not perfect ranking)
```

## ðŸš¨ Troubleshooting

**No evaluation results?**
â†’ Run: `python src/evals/eval_runner.py --batch`

**API returns 404?**
â†’ Check: `ls data/eval/results.json`

**Streamlit not showing data?**
â†’ Clear cache and refresh

**MRR scores too low?**
â†’ Review fact extraction in `eval_runner.py`

## ðŸ“ž Support

- Check EVALUATION_FRAMEWORK_README.md for detailed guide
- Review examples in eval_metrics.py
- Run with verbose: `VERBOSE=1 python src/evals/eval_runner.py --company world-labs`
