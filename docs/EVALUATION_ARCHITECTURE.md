# Evaluation Framework - Architecture & Data Flow

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Evaluation Framework Architecture                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Ground Truth   â”‚
                         â”‚  Data (JSON)    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚             â”‚
                    â–¼             â–¼             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Structured      â”‚ â”‚ RAG             â”‚ â”‚ Reference       â”‚
        â”‚ Dashboard       â”‚ â”‚ Dashboard       â”‚ â”‚ Materials       â”‚
        â”‚ Markdown        â”‚ â”‚ Markdown        â”‚ â”‚ (URLs, Facts)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚                   â”‚
                 â”‚                    â”‚                   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Evaluation Runner    â”‚
                          â”‚  (eval_runner.py)     â”‚
                          â”‚  - Extract facts      â”‚
                          â”‚  - Calculate metrics  â”‚
                          â”‚  - Score output       â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Evaluation Metrics Module      â”‚
                    â”‚   (eval_metrics.py)              â”‚
                    â”‚                                  â”‚
                    â”‚  â€¢ EvaluationMetrics class       â”‚
                    â”‚  â€¢ calculate_mrr()               â”‚
                    â”‚  â€¢ ComparisonResult class        â”‚
                    â”‚  â€¢ Total score calculation       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                         â”‚
                    â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Results Cache         â”‚  â”‚ Comparison Report     â”‚
        â”‚ (results.json)        â”‚  â”‚ (report.md)           â”‚
        â”‚ - Structured scores   â”‚  â”‚ - Table comparison    â”‚
        â”‚ - RAG scores          â”‚  â”‚ - Summary statistics  â”‚
        â”‚ - Winners per metric  â”‚  â”‚ - MRR analysis        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        â”‚        â”‚
        â–¼        â–¼        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Multiple Access Points                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. CLI (eval_runner.py)                     â”‚
    â”‚ 2. API (/evals/{company_slug})             â”‚
    â”‚ 3. Streamlit Dashboard                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Processing Pipeline                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT
  â”‚
  â”œâ”€ Ground Truth (data/eval/ground_truth.json)
  â”‚  â””â”€ Official sources
  â”‚  â””â”€ Key facts
  â”‚  â””â”€ Hallucination examples
  â”‚
  â”œâ”€ Generated Dashboard (Markdown)
  â”‚  â””â”€ Structured Pipeline
  â”‚  â””â”€ RAG Pipeline
  â”‚
  â””â”€ Reference Material (URLs, fact sources)


PROCESSING (EvaluationRunner)
  â”‚
  â”œâ”€ Load & validate ground truth
  â”œâ”€ Extract facts from dashboard markdown
  â”œâ”€ Assign relevance scores to facts
  â”œâ”€ Calculate MRR (fact ranking quality)
  â”œâ”€ Score other metrics against ground truth:
  â”‚  â”œâ”€ Factual accuracy (0-3)
  â”‚  â”œâ”€ Schema compliance (0-2)
  â”‚  â”œâ”€ Provenance quality (0-2)
  â”‚  â”œâ”€ Hallucination detection (0-2)
  â”‚  â”œâ”€ Readability (0-1)
  â”‚  â””â”€ MRR (0-1)
  â”œâ”€ Calculate total score (0-14)
  â””â”€ Compare structured vs RAG


OUTPUT
  â”‚
  â”œâ”€ Cache Results (data/eval/results.json)
  â”‚  â””â”€ Structured pipeline metrics
  â”‚  â””â”€ RAG pipeline metrics
  â”‚  â””â”€ Winner per metric
  â”‚
  â”œâ”€ Generate Report (data/eval/report.md)
  â”‚  â””â”€ Comparison table
  â”‚  â””â”€ Summary statistics
  â”‚  â””â”€ MRR analysis
  â”‚
  â””â”€ Return to User
     â”œâ”€ CLI display
     â”œâ”€ API response
     â””â”€ Streamlit visualization
```

## ğŸ”„ Evaluation Process Flow

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. PREPARE GROUND TRUTH                 â”‚
â”‚                                         â”‚
â”‚ User edits data/eval/ground_truth.json  â”‚
â”‚ - Add company info                      â”‚
â”‚ - Add official sources                  â”‚
â”‚ - Add key facts                         â”‚
â”‚ - Add hallucination examples            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. GENERATE DASHBOARDS                  â”‚
â”‚                                         â”‚
â”‚ For each company, generate:             â”‚
â”‚ - Structured pipeline output            â”‚
â”‚ - RAG pipeline output                   â”‚
â”‚                                         â”‚
â”‚ Methods:                                â”‚
â”‚ - API: POST /dashboard/structured       â”‚
â”‚ - API: POST /dashboard/rag              â”‚
â”‚ - Streamlit: Click buttons              â”‚
â”‚ - Save outputs for reference            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. RUN EVALUATION                       â”‚
â”‚                                         â”‚
â”‚ python src/evals/eval_runner.py --..    â”‚
â”‚                                         â”‚
â”‚ Process:                                â”‚
â”‚ - Load ground truth for company         â”‚
â”‚ - Load dashboard markdown               â”‚
â”‚ - Extract facts from markdown           â”‚
â”‚ - Compare against ground truth          â”‚
â”‚ - Calculate metrics                     â”‚
â”‚ - Cache results                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RESULTS STORED & CACHED              â”‚
â”‚                                         â”‚
â”‚ Results saved to:                       â”‚
â”‚ data/eval/results.json                  â”‚
â”‚                                         â”‚
â”‚ Structure:                              â”‚
â”‚ {                                       â”‚
â”‚   "company-slug": {                     â”‚
â”‚     "structured": { metrics },          â”‚
â”‚     "rag": { metrics }                  â”‚
â”‚   }                                     â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. VIEW & ANALYZE RESULTS               â”‚
â”‚                                         â”‚
â”‚ Access results via:                     â”‚
â”‚ - CLI: eval_runner.py --view            â”‚
â”‚ - API: GET /evals/{company_slug}       â”‚
â”‚ - Streamlit: Dashboard UI               â”‚
â”‚                                         â”‚
â”‚ Features:                               â”‚
â”‚ - Comparison tables                     â”‚
â”‚ - Charts & visualizations               â”‚
â”‚ - MRR analysis                          â”‚
â”‚ - Batch summary                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. GENERATE REPORT (Optional)           â”‚
â”‚                                         â”‚
â”‚ python src/evals/eval_runner.py \       â”‚
â”‚   --batch --report                      â”‚
â”‚                                         â”‚
â”‚ Output: data/eval/report.md             â”‚
â”‚ - Markdown table                        â”‚
â”‚ - Summary statistics                    â”‚
â”‚ - MRR analysis                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
               END
```

## ğŸ¯ Metrics Calculation Flow

```
Dashboard Markdown
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract Facts                        â”‚
â”‚ - Parse markdown lines               â”‚
â”‚ - Extract key information            â”‚
â”‚ - Assign initial relevance scores    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compare with Ground Truth            â”‚
â”‚ - Verify factual accuracy            â”‚
â”‚ - Check for hallucinations           â”‚
â”‚ - Validate citations                 â”‚
â”‚ - Assess information completeness    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚          â”‚
        â–¼          â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Score  â”‚ â”‚ Score  â”‚ â”‚ Score  â”‚
    â”‚ (0-3)  â”‚ â”‚ (0-2)  â”‚ â”‚ (0-1)  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Fact.  â”‚ â”‚Schema  â”‚ â”‚ Read.  â”‚
    â”‚ Accu.  â”‚ â”‚Compl.  â”‚ â”‚        â”‚
    â”‚ + Hall.â”‚ â”‚ + Prov.â”‚ â”‚+ othersâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚         â”‚         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calculate MRR                        â”‚
â”‚                                      â”‚
â”‚ Facts: [                             â”‚
â”‚   {"relevance": 0.95, "rank": 1},   â”‚
â”‚   {"relevance": 0.50, "rank": 2},   â”‚
â”‚ ]                                    â”‚
â”‚                                      â”‚
â”‚ MRR = 1/1 = 1.00 (first is most rel)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calculate Total Score                â”‚
â”‚                                      â”‚
â”‚ Total = Fact (3)                     â”‚
â”‚       + Schema (2)                   â”‚
â”‚       + Prov (2)                     â”‚
â”‚       + Hall (2)                     â”‚
â”‚       + Read (1)                     â”‚
â”‚       + MRR*2 (0-2)                  â”‚
â”‚       = 0-14                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
            EvaluationMetrics
            {
              factual_accuracy: 3,
              schema_compliance: 2,
              provenance_quality: 2,
              hallucination_detection: 2,
              readability: 1,
              mrr_score: 0.95,
              total_score: 13.9
            }
```

## ğŸ”— Component Interactions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  System Components                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  eval_metrics.py       â”‚  (Core Logic)
â”‚                        â”‚
â”‚ â€¢ EvaluationMetrics    â”‚
â”‚ â€¢ ComparisonResult     â”‚
â”‚ â€¢ calculate_mrr()      â”‚
â”‚ â€¢ validate()           â”‚
â”‚ â€¢ get_total_score()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ (imports & uses)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  eval_runner.py        â”‚  (Orchestration)
â”‚                        â”‚
â”‚ â€¢ EvaluationRunner     â”‚
â”‚ â€¢ evaluate_company()   â”‚
â”‚ â€¢ batch_evaluate()     â”‚
â”‚ â€¢ generate_report()    â”‚
â”‚ â€¢ CLI interface        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ (uses & caches)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backend       â”‚  (API Layer)
â”‚  (rag_search_api.py)   â”‚
â”‚                        â”‚
â”‚ â€¢ EvaluationMetrics    â”‚
â”‚   Response (model)     â”‚
â”‚ â€¢ /evals/{slug}        â”‚
â”‚   (endpoint)           â”‚
â”‚ â€¢ /evals               â”‚
â”‚   (endpoint)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ (returns JSON)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit Frontend    â”‚  (UI Layer)
â”‚  (eval_dashboard.py)   â”‚
â”‚                        â”‚
â”‚ â€¢ Company selector     â”‚
â”‚ â€¢ Comparison table     â”‚
â”‚ â€¢ Radar chart          â”‚
â”‚ â€¢ Bar chart            â”‚
â”‚ â€¢ MRR analysis         â”‚
â”‚ â€¢ Batch summary        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Score Calculation Example

```
PIPELINE: Structured (World Labs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Dashboard Content:
â”Œâ”€ Company Founded: 2023
â”œâ”€ Funding: $230M Series Unknown
â”œâ”€ Investors: NEA, a16z
â”œâ”€ Location: San Francisco
â”œâ”€ Products: Large World Models
â””â”€ Headcount: 38 employees


SCORING PROCESS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. FACTUAL ACCURACY (0-3)
   â”œâ”€ "Founded 2023": âœ“ Correct (ground truth)
   â”œâ”€ "$230M funding": âœ“ Correct (ground truth)
   â”œâ”€ "NEA, a16z": âœ“ Correct (ground truth)
   â”œâ”€ "San Francisco": âœ“ Correct (ground truth)
   â””â”€ All facts verified â†’ Score: 3/3

2. SCHEMA COMPLIANCE (0-2)
   â”œâ”€ All required sections: âœ“
   â”œâ”€ Proper formatting: âœ“
   â””â”€ â†’ Score: 2/2

3. PROVENANCE QUALITY (0-2)
   â”œâ”€ Citations for funding: âœ“
   â”œâ”€ Sources cited: âœ“
   â””â”€ â†’ Score: 2/2

4. HALLUCINATION DETECTION (0-2)
   â”œâ”€ No false claims: âœ“
   â””â”€ â†’ Score: 2/2

5. READABILITY (0-1)
   â”œâ”€ Clear structure: âœ“
   â””â”€ â†’ Score: 1/1

6. MEAN RECIPROCAL RANKING (0-1)
   â”œâ”€ Most relevant fact (funding): Position 1
   â”œâ”€ MRR = 1/1 = 1.00
   â””â”€ â†’ Score: 0.95 (slight adjustment)


TOTAL SCORE CALCULATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Factual Accuracy:       3
Schema Compliance:      2
Provenance Quality:     2
Hallucination Detect:   2
Readability:            1
MRR (scaled 0-2):     1.9 (0.95 * 2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:             13.9 / 14


INTERPRETATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Score: 13.9 / 14
Percentage: 99%
Rating: â­â­â­â­â­ EXCELLENT
Status: Production Ready
```

## ğŸ¯ MRR Ranking Example

```
Dashboard Output Order:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. "World Labs raised $230M in Series Unknown"
   â””â”€ Relevance: 0.95 â† MOST RELEVANT (funding info)
      Rank: 1

2. "Founded in 2023, headquarters in San Francisco"
   â””â”€ Relevance: 0.85 (important but secondary)
      Rank: 2

3. "Large World Models (LWMs) technology"
   â””â”€ Relevance: 0.70 (supporting info)
      Rank: 3


MRR CALCULATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

First relevant fact (â‰¥ 0.7 threshold):
â”œâ”€ Fact 1: relevance 0.95 â‰¥ 0.7 âœ“
â””â”€ This is the first relevant fact

MRR = 1 / rank_of_first_relevant
    = 1 / 1
    = 1.00 (perfect ranking)

But adjusted for actual relevance score:
MRR = min(0.95, 1.0) = 0.95


ALTERNATIVE SCENARIO (Sub-optimal)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

If dashboard had organized differently:

1. "Historical background of AI emergence"
   â””â”€ Relevance: 0.30 (not relevant)

2. "Industry comparison overview"
   â””â”€ Relevance: 0.40 (not relevant)

3. "World Labs: $230M funding"
   â””â”€ Relevance: 0.95 â† MOST RELEVANT (but third!)
      Rank: 3

First relevant fact:
â”œâ”€ Fact 1: relevance 0.30 < 0.7 âœ—
â”œâ”€ Fact 2: relevance 0.40 < 0.7 âœ—
â””â”€ Fact 3: relevance 0.95 â‰¥ 0.7 âœ“

MRR = 1 / 3 = 0.33 (poor ranking)

COMPARISON
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Good ranking (MRR 0.95): Important info first
Poor ranking (MRR 0.33): Important info buried
â†’ Structured pipeline (0.95) >> RAG pipeline (0.33)
```

## ğŸ’¾ Storage & Caching

```
REQUEST FLOW WITH CACHING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Request
     â”‚
     â–¼
Check Cache (data/eval/results.json)
     â”‚
     â”œâ”€ YES: Return cached result
     â”‚
     â””â”€ NO: 
        â”‚
        â–¼
     Load Ground Truth
     â”‚
     â–¼
     Extract & Score Facts
     â”‚
     â–¼
     Calculate Metrics
     â”‚
     â–¼
     Cache Result
     â”‚
     â–¼
     Return Result to User


CACHE STRUCTURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

data/eval/results.json
{
  "world-labs": {
    "structured": {
      "company_name": "World Labs",
      "pipeline_type": "structured",
      "timestamp": "2025-11-06T10:30:00Z",
      "factual_accuracy": 3,
      "schema_compliance": 2,
      "provenance_quality": 2,
      "hallucination_detection": 2,
      "readability": 1,
      "mrr_score": 0.95,
      "total_score": 13.9,
      "notes": "Excellent output"
    },
    "rag": {
      "company_name": "World Labs",
      "pipeline_type": "rag",
      "timestamp": "2025-11-06T10:35:00Z",
      "factual_accuracy": 2,
      "schema_compliance": 2,
      "provenance_quality": 1,
      "hallucination_detection": 1,
      "readability": 1,
      "mrr_score": 0.75,
      "total_score": 10.5,
      "notes": "Good but some hallucinations"
    }
  }
}
```

---

**Complete architecture and data flow of the evaluation framework!**
