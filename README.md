# ğŸš€ Project ORBIT (Part 2): Agentification and Secure Scaling of PE Intelligence
**DAMG 7245 â€“ Fall 2025 â€“ Assignment 5 â€“ Binary Insights**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Airflow](https://img.shields.io/badge/Airflow-2.10.4-orange.svg)](https://airflow.apache.org)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![LangChain](https://img.shields.io/badge/LangChain-1.0+-green.svg)](https://langchain.com)
[![MCP](https://img.shields.io/badge/MCP-Server-purple.svg)](https://modelcontextprotocol.io)

## ğŸ“‹ Overview

**Project ORBIT Part 2** evolves the static PE intelligence platform from Assignment 4 into an **agentic, production-ready system** that orchestrates due-diligence workflows through supervisory LLM agents using the **Model Context Protocol (MCP)**.

### ğŸ¯ What's New in Part 2?

This project extends Assignment 4 with **autonomous agent capabilities**:

- ğŸ¤– **Two Specialized Agents**:
  - **Tavily Agent** - Web search intelligence using Tavily API
  - **Payload Agent** - RAG-based entity extraction from Pinecone
  
- ğŸ­ **Master Orchestrator** - Supervisory agent coordinating both sub-agents with unified logging and versioning

- ğŸ”§ **MCP Server** (Port 9000) - Model Context Protocol server with:
  - 4 Tools (search_company, extract_field, enrich_payload, analyze_null_fields)
  - 2 Resources (/ai50/companies, /company/{id}/info)
  - 2 Prompts (/prompt/pe-dashboard, /prompt/rag-dashboard)
  - 4-Layer Security (whitelist, RBAC, rate limiting, input validation)

- ğŸ§  **ReAct Pattern** - Structured Thought â†’ Action â†’ Observation logging with JSON traces

- ğŸ”€ **LangGraph Workflows** - State machines for both agents with conditional edges

- ğŸ“Š **12 Airflow DAGs** - Complete pipeline orchestration including:
  - `master_orchestrator_dag` - Sequential pipeline execution
  - `master_agent_dag` - Batch agent enrichment
  - `dashboard_generation_dag` - Parallel dashboard generation
  - Individual DAGs for scraping, ingestion, extraction, evaluation

- âœ… **87+ Tests** - Comprehensive pytest suite:
  - `test_tavily_agent.py` - 25+ tests
  - `test_payload_workflow.py` - Integration tests
  - `test_mcp_server.py` - Security and API tests

- ğŸ³ **Production Deployment** - Full Docker stack on AWS EC2 (98.95.70.0)

---

## ğŸ—ï¸ System Architecture

![Project ORBIT Architecture](docs/architecture_diagram.png)

---

## Graph Workflow

![Tavily Agent](graph/graph_visualization.png)

![Payload Agent](graph/payload_workflow_graph.png)

## ğŸ“¦ Quick Start

### ğŸš€ Option A: Use Deployed System (Fastest)

Access the production deployment on AWS EC2:

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow UI** | http://98.95.70.0:8080 | `airflow` / `airflow` |
| **MCP Server** | http://98.95.70.0:9000 | N/A (API) |
| **FastAPI Docs** | http://98.95.70.0:8000/docs | N/A |
| **Streamlit Dashboard** | http://98.95.70.0:8501 | N/A |

### ğŸ› ï¸ Option B: Local Development Setup

#### 1ï¸âƒ£ Prerequisites

- Python 3.11+
- Docker 20.10+ & Docker Compose 2.0+
- Git (latest)
- **Required API Keys**:
  - OpenAI API key
  - Pinecone API key
  - Tavily API key (for Tavily Agent)
- AWS credentials (optional, for S3)

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Binary-Insights/Assignment_05.git
cd Assignment_05
```

---

### 3ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the project root:

```bash
# Copy example configuration
cp docker/.env.example docker/.env
```

Edit `docker/.env` with your credentials:

```bash
# ===== REQUIRED: OpenAI Configuration =====
OPENAI_API_KEY=sk-proj-...your-key-here...

# ===== REQUIRED: Pinecone Configuration =====
PINECONE_API_KEY=pcsk_...your-key-here...
PINECONE_INDEX_NAME=bigdata-assignment-05
PINECONE_NAMESPACE=default
PINECONE_EMBEDDING_DIMENSION=3072
PINECONE_EMBEDDING_MODEL=text-embedding-3-large

# ===== REQUIRED: Tavily API (for Tavily Agent) =====
TAVILY_API_KEY=tvly-...your-key-here...

# ===== OPTIONAL: AWS S3 Configuration =====
AWS_ACCESS_KEY_ID=AKIA...your-key...
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET_NAME=pe-dashboard-ai50

# ===== OPTIONAL: LangSmith Tracing =====
LANGSMITH_API_KEY=lsv2_pt_...your-key...
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=orbit-assignment-05

# ===== Airflow Configuration =====
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true

# ===== Application Configuration =====
LOG_LEVEL=INFO
PROJECT_NAME=Assignment_05
FASTAPI_URL=http://fastapi:8000
```

---

### 4ï¸âƒ£ Local Development Setup (Optional)

For local development without Docker:

```bash
# Create virtual environment
python -m venv .venv
# or with uv (faster):
uv venv .venv

# Activate virtual environment
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
# or with uv:
uv sync
```

---

### 5ï¸âƒ£ Start Services with Docker

```bash
# Navigate to docker directory
cd docker

# Build images (first time or after code changes)
docker-compose build --no-cache

# Start all services
docker-compose up -d

# Wait 30-60 seconds for Airflow initialization

# Verify services are running
docker ps
```

**Important Note on Data Directory Permissions:**

The `data/` directory is owned by Airflow user (uid 50000). If you need to modify files:

```bash
# To edit files manually
sudo chown -R ubuntu:ubuntu /home/ubuntu/Assignment_05/data

# Before running Airflow tasks, restore permissions
sudo chown -R 50000:root /home/ubuntu/Assignment_05/data
```

---

### 6ï¸âƒ£ Run Agents Locally (Without Docker)

```bash
# Activate virtual environment
source .venv/bin/activate

# Run Tavily Agent (web search)
python src/tavily_agent/main.py abridge

# Run Payload Agent (RAG extraction)
python -c "
from payload_agent import PayloadAgent
agent = PayloadAgent()
result = agent.retrieve_and_validate('abridge', use_agent=True)
print(f'Status: {result[\"status\"]}')
"

# Run Master Orchestrator (both agents)
python src/master_agent/master.py abridge

# Run with options
python src/master_agent/master.py abridge --verbose
python src/master_agent/master.py abridge --tavily-only
python src/master_agent/master.py abridge --payload-only
```

---

### 7ï¸âƒ£ Run Tests

```bash
# Run all tests
pytest -v

# Run specific test suite
pytest tests/test_tavily_agent.py -v
pytest tests/test_payload_workflow.py -v
pytest tests/test_mcp_server.py -v

# Run with coverage
pytest --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html
```

---

### 8ï¸âƒ£ Access Web Interfaces

| Service | Local URL | EC2 URL | Credentials |
|---------|-----------|---------|-------------|
| **Airflow UI** | http://localhost:8080 | http://98.95.70.0:8080 | `airflow` / `airflow` |
| **MCP Server** | http://localhost:9000 | http://98.95.70.0:9000 | N/A (API) |
| **FastAPI Docs** | http://localhost:8000/docs | http://98.95.70.0:8000/docs | N/A |
| **Streamlit Dashboard** | http://localhost:8501 | http://98.95.70.0:8501 | N/A 


## ğŸ“‚ Project Structure

```
Assignment_05/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“ tavily_agent/              # ğŸ¤– Agent 1: Web Search Intelligence
â”‚   â”‚   â”œâ”€â”€ main.py                   # AgenticRAGOrchestrator
â”‚   â”‚   â”œâ”€â”€ graph.py                  # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ tools.py                  # Tavily search tool
â”‚   â”‚   â”œâ”€â”€ llm_extraction.py         # LLM extraction chain
â”‚   â”‚   â”œâ”€â”€ file_io_manager.py        # File operations
â”‚   â”‚   â””â”€â”€ README.md                 # Documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ payload_agent/             # ğŸ¤– Agent 2: RAG Entity Extraction
â”‚   â”‚   â”œâ”€â”€ payload_agent.py          # Main agent class
â”‚   â”‚   â”œâ”€â”€ payload_workflow.py       # LangGraph state machine
â”‚   â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py          # @tool get_latest_structured_payload
â”‚   â”‚   â”‚   â”œâ”€â”€ validation.py         # @tool validate_payload
â”‚   â”‚   â”‚   â””â”€â”€ rag_adapter.py        # Pinecone adapter
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ master_agent/              # ğŸ­ Master Orchestrator
â”‚   â”‚   â”œâ”€â”€ master.py                 # Coordinates Tavily + Payload agents
â”‚   â”‚   â”œâ”€â”€ QUICK_REFERENCE.md
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ mcp_server/                # ğŸ”§ Model Context Protocol Server
â”‚   â”‚   â”œâ”€â”€ server.py                 # FastMCP server (port 9000)
â”‚   â”‚   â”œâ”€â”€ security.py               # 4-layer security middleware
â”‚   â”‚   â”œâ”€â”€ mcp_enrichment_client.py  # MCP client wrapper
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ dags/                      # ğŸ”„ Airflow DAG Definitions
â”‚   â”‚   â”œâ”€â”€ master_orchestrator_dag.py  # Sequential pipeline (6 DAGs)
â”‚   â”‚   â”œâ”€â”€ master_agent_dag.py         # Batch agent enrichment
â”‚   â”‚   â”œâ”€â”€ process_pages_dag.py        # Web scraping (Selenium)
â”‚   â”‚   â”œâ”€â”€ ingest_dag.py               # Pinecone ingestion
â”‚   â”‚   â”œâ”€â”€ extraction_dag.py           # Structured extraction
â”‚   â”‚   â”œâ”€â”€ dashboard_generation_dag.py # Generate dashboards
â”‚   â”‚   â”œâ”€â”€ eval_runner_dag.py          # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ discover_dag.py             # Company discovery
â”‚   â”‚   â”œâ”€â”€ storing_dag.py              # S3 storage
â”‚   â”‚   â”œâ”€â”€ agentic_rag_dag.py          # Legacy Tavily DAG
â”‚   â”‚   â”œâ”€â”€ payload_agent_dag.py        # Legacy Payload DAG
â”‚   â”‚   â””â”€â”€ enrichment_dag.py           # Legacy enrichment
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ rag/                       # ğŸ” RAG Pipeline (Assignment 4)
â”‚   â”‚   â”œâ”€â”€ ingest_to_pinecone.py     # Vector ingestion
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py           # RAG search
â”‚   â”‚   â”œâ”€â”€ structured_extraction.py  # Entity extraction
â”‚   â”‚   â””â”€â”€ rag_models.py             # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ discover/                  # ğŸŒ Web Scraping
â”‚   â”‚   â”œâ”€â”€ discover.py               # Company discovery
â”‚   â”‚   â””â”€â”€ process_discovered_pages.py # HTML download (Selenium)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ backend/                   # ğŸš€ FastAPI Backend
â”‚   â”‚   â””â”€â”€ rag_search_api.py         # REST endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ frontend/                  # ğŸ¨ Streamlit Frontend
â”‚   â”‚   â”œâ”€â”€ streamlit_app.py          # Dashboard UI
â”‚   â”‚   â””â”€â”€ eval_dashboard.py         # Evaluation UI
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ evals/                     # âœ… Evaluation Framework
â”‚   â”‚   â”œâ”€â”€ result_evaluator.py       # Quality scoring
â”‚   â”‚   â”œâ”€â”€ eval_runner.py            # Evaluation runner
â”‚   â”‚   â””â”€â”€ eval_metrics.py           # Metrics calculation
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ prompts/                   # ğŸ“ LLM Prompts
â”‚       â””â”€â”€ pe_dashboard.md           # Dashboard template
â”‚
â”œâ”€â”€ ğŸ“ tests/                         # âœ… Test Suite (87+ tests)
â”‚   â”œâ”€â”€ test_tavily_agent.py          # Tavily agent tests (25+)
â”‚   â”œâ”€â”€ test_payload_tools.py         # Tool validation tests
â”‚   â”œâ”€â”€ test_payload_workflow.py      # Workflow integration tests
â”‚   â””â”€â”€ test_mcp_server.py            # MCP server + security tests
â”‚
â”œâ”€â”€ ğŸ“ docker/                        # ğŸ³ Docker Deployment
â”‚   â”œâ”€â”€ Dockerfile                    # Multi-stage image
â”‚   â”œâ”€â”€ docker-compose.yml            # Service orchestration
â”‚   â”œâ”€â”€ .env.example                  # Environment template
â”‚   â””â”€â”€ README.md                     # Docker guide
â”‚
â”œâ”€â”€ ğŸ“ data/                          # ğŸ’¾ Data Storage
â”‚   â”œâ”€â”€ payloads/                     # Company JSON payloads
â”‚   â”œâ”€â”€ raw/                          # Scraped HTML/text
â”‚   â”œâ”€â”€ logs/                         # ReAct traces
â”‚   â””â”€â”€ llm_response/                 # Generated dashboards
â”‚
â”œâ”€â”€ ğŸ“ docs/                          # ğŸ“š Documentation
â”‚   â”œâ”€â”€ 00_START_HERE.md
â”‚   â”œâ”€â”€ EVALUATION_GUIDE.md
â”‚   â””â”€â”€ ENV_CONFIGURATION_GUIDE.md
â”‚
â”œâ”€â”€ CODELABS.md                       # ğŸ“– Step-by-step tutorial
â”œâ”€â”€ README.md                         # ğŸ‘ˆ You are here
â”œâ”€â”€ Assignment5.md                    # ğŸ“‹ Requirements
â”œâ”€â”€ requirements.txt                  # ğŸ“¦ Dependencies
â””â”€â”€ pyproject.toml                    # ğŸ”§ Project config
```

---

---

## ğŸš€ Usage Examples

### Running Individual Agents

```bash
# Tavily Agent - Web search for company data
python src/tavily_agent/main.py abridge
python src/tavily_agent/main.py "world labs" anthropic

# Payload Agent - RAG-based entity extraction
python -c "
from payload_agent import PayloadAgent
agent = PayloadAgent()
result = agent.retrieve_and_validate('abridge', use_agent=True)
print(result)
"

# Master Orchestrator - Run both agents
python src/master_agent/master.py abridge --verbose
python src/master_agent/master.py abridge --tavily-only
python src/master_agent/master.py abridge --payload-only
```

### Running Airflow DAGs

```bash
# Trigger master orchestrator (runs all 6 DAGs sequentially)
docker exec assignment04-airflow-scheduler \
  airflow dags trigger master_orchestrator_dag

# Trigger individual DAGs
docker exec assignment04-airflow-scheduler \
  airflow dags trigger process_pages_dag

docker exec assignment04-airflow-scheduler \
  airflow dags trigger master_agent_dag

docker exec assignment04-airflow-scheduler \
  airflow dags trigger dashboard_generation_dag

# View DAG status
docker exec assignment04-airflow-scheduler \
  airflow dags list
```

### MCP Server Usage

```python
from mcp_server.mcp_enrichment_client import MCPEnrichmentClient

# Create client
client = MCPEnrichmentClient()

# Search company
results = await client.search_company(
    company_name="Abridge",
    query="funding rounds and valuation"
)

# Extract specific field
value = await client.extract_field(
    field_name="founded_year",
    company_name="Abridge",
    search_results=results
)

# Full enrichment (requires admin role)
enriched = await client.enrich_payload(
    company_name="Abridge"
)
```

---

## ğŸ”— Documentation Links

ğŸ“š **[Full Technical Codelabs](https://codelabs-preview.appspot.com/?file_id=1ibeTrsOHlACsSpoRAOsVegIjVwM7bivPmKhUL0MBTd0)** â€” Complete step-by-step guide (500+ lines)  
ğŸ“‹ **[Assignment Requirements](./Assignment5.md)** â€” Lab breakdown & requirements  
ğŸ¥ **[Demo Video]()** â€” Project walkthrough (TBD)  


---

## ğŸ¯ Key Features

### ğŸ¤– Autonomous Agents

- **Tavily Agent**: Web search via Tavily API with LangGraph workflow
  - Enriches company_record fields (founded_year, hq_city, funding, etc.)
  - ReAct reasoning with structured logging
  - Automatic versioning (v1, v2, v3...)

- **Payload Agent**: RAG-based entity extraction from Pinecone
  - Extracts events, products, leadership, snapshots
  - 3 execution modes (autonomous, manual ReAct, direct)
  - LangGraph state machine with conditional edges

- **Master Orchestrator**: Coordinates both agents
  - Two-phase enrichment workflow
  - Unified logging and error handling
  - Comprehensive statistics and reporting

### ğŸ”§ MCP Server (Port 9000)

- **4 Tools**: search_company, extract_field, enrich_payload, analyze_null_fields
- **2 Resources**: /ai50/companies, /company/{id}/info
- **2 Prompts**: /prompt/pe-dashboard, /prompt/rag-dashboard
- **4-Layer Security**:
  1. Tool whitelist filtering
  2. Role-based access control
  3. Rate limiting (per-tool)
  4. Input validation (SQL injection, XSS, etc.)

### ğŸ”„ Airflow Orchestration

- **12 DAGs** for complete pipeline automation
- **Master Orchestrator DAG**: Sequential execution of 6 sub-DAGs
- **Dynamic task generation**: Processes all 50 companies in parallel
- **Error handling**: Retries with exponential backoff

### âœ… Testing & Quality

- **87+ pytest tests** across 4 test suites
- **Test coverage**: 85%+ of agent code
- **Security tests**: SQL injection, XSS, rate limiting, RBAC
- **Integration tests**: End-to-end workflow validation

---