# ğŸš€ Project ORBIT (Part 2): Agentification and Secure Scaling of PE Intelligence
**DAMG 7245 â€“ Fall 2025 â€“ Assignment 5 â€“ Binary Insights**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Airflow](https://img.shields.io/badge/Airflow-2.10.4-orange.svg)](https://airflow.apache.org)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![LangChain](https://img.shields.io/badge/LangChain-1.0+-green.svg)](https://langchain.com)
[![MCP](https://img.shields.io/badge/MCP-Server-purple.svg)](https://modelcontextprotocol.io)

## ğŸ“‹ Overview

**Project ORBIT Part 2** evolves the static PE intelligence platform from Assignment 4 into an **agentic, production-ready system** that orchestrates due-diligence workflows through supervisory LLM agents using the **Model Context Protocol (MCP)**.

The system features:
- ğŸ¤– **Supervisory Agent Architecture** with specialized sub-agents (Planner, Evaluator, Risk Detector)
- ğŸ”§ **Model Context Protocol (MCP)** server exposing Tools, Prompts, and Resources
- ğŸ§  **ReAct Pattern** implementation with structured Thought â†’ Action â†’ Observation logging
- ğŸ”€ **LangGraph Workflow** with conditional branching and Human-in-the-Loop (HITL) approval
- ğŸ“Š **Dual Dashboard Generation** using RAG and Structured Extraction pipelines
- ğŸ³ **Full Docker Deployment** with Airflow orchestration
- âœ… **Comprehensive Testing** with pytest coverage

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    subgraph "Airflow Orchestration Layer"
        DAG1[Initial Load DAG<br/>Data Discovery & Setup]
        DAG2[Daily Update DAG<br/>Incremental Updates]
        DAG3[Agentic Dashboard DAG<br/>Agent Workflow Execution]
        DAG4[Master Orchestrator<br/>Pipeline Coordination]
    end
    
    subgraph "MCP Server Layer"
        MCP[MCP Server :9000<br/>Tools | Prompts | Resources]
        TOOL1[generate_structured_dashboard]
        TOOL2[generate_rag_dashboard]
        RES1[/resource/ai50/companies]
        PROMPT1[/prompt/pe-dashboard]
    end
    
    subgraph "Agent Layer"
        SUPER[Supervisor Agent<br/>ReAct Orchestration]
        PLANNER[Planner Agent<br/>Task Planning]
        EVAL[Evaluation Agent<br/>Quality Scoring]
        RISK[Risk Detector<br/>Signal Analysis]
    end
    
    subgraph "Workflow Layer"
        GRAPH[LangGraph Workflow]
        NODE1[Plan Generation]
        NODE2[Data Collection]
        NODE3[Dashboard Generation]
        NODE4[Quality Evaluation]
        NODE5{Risk Detection}
        HITL[Human Approval<br/>HITL Pause]
        AUTO[Auto-Approve]
    end
    
    subgraph "Data Layer"
        PINECONE[(Pinecone<br/>Vector DB)]
        S3[(AWS S3<br/>Cloud Storage)]
        PAYLOADS[(Local JSON<br/>Payloads)]
        LOGS[(Structured Logs<br/>ReAct Traces)]
    end
    
    subgraph "Interface Layer"
        FASTAPI[FastAPI :8000<br/>REST API]
        STREAMLIT[Streamlit :8501<br/>Dashboard UI]
    end
    
    DAG3 -->|HTTP/CLI| MCP
    DAG4 --> DAG1 & DAG2 & DAG3
    MCP --> TOOL1 & TOOL2 & RES1 & PROMPT1
    MCP --> SUPER
    SUPER --> PLANNER & EVAL & RISK
    SUPER --> GRAPH
    GRAPH --> NODE1 --> NODE2 --> NODE3 --> NODE4 --> NODE5
    NODE5 -->|Risk Found| HITL --> PAYLOADS
    NODE5 -->|No Risk| AUTO --> PAYLOADS
    TOOL1 & TOOL2 --> PINECONE & PAYLOADS
    PAYLOADS --> S3
    SUPER --> LOGS
    FASTAPI --> PINECONE & PAYLOADS
    STREAMLIT --> FASTAPI
    
    classDef airflowStyle fill:#f9f,stroke:#333,stroke-width:2px
    classDef mcpStyle fill:#bbf,stroke:#333,stroke-width:2px
    classDef agentStyle fill:#bfb,stroke:#333,stroke-width:2px
    classDef workflowStyle fill:#ffb,stroke:#333,stroke-width:2px
    classDef dataStyle fill:#fbb,stroke:#333,stroke-width:2px
    
    class DAG1,DAG2,DAG3,DAG4 airflowStyle
    class MCP,TOOL1,TOOL2,RES1,PROMPT1 mcpStyle
    class SUPER,PLANNER,EVAL,RISK agentStyle
    class GRAPH,NODE1,NODE2,NODE3,NODE4,NODE5,HITL,AUTO workflowStyle
    class PINECONE,S3,PAYLOADS,LOGS dataStyle
```

---

## ğŸ“¦ Setup Instructions

### 1ï¸âƒ£ Prerequisites

- Python 3.11+
- Docker 20.10+
- Docker Compose 2.0+
- Git (latest)
- OpenAI API key (required)
- Pinecone API key (required)
- Airflow 2.10+
- AWS credentials (S3,EC2)

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Binary-Insights/Assignment_05.git
cd Assignment_05
```

---

### 3ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the project root:

```bash
# Copy example configuration
cp .env.example .env
```

Edit `.env` with your credentials:

```bash
# ===== REQUIRED: OpenAI Configuration =====
OPENAI_API_KEY=sk-proj-...your-key-here...

# ===== REQUIRED: Pinecone Configuration =====
PINECONE_API_KEY=pcsk_...your-key-here...
PINECONE_INDEX_NAME=bigdata-assignment-05
PINECONE_NAMESPACE=default
PINECONE_EMBEDDING_DIMENSION=3072
PINECONE_EMBEDDING_MODEL=text-embedding-3-large

# ===== OPTIONAL: AWS S3 Configuration =====
AWS_ACCESS_KEY_ID=AKIA...your-key...
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET_NAME=pe-dashboard-ai50

# ===== OPTIONAL: LangSmith Tracing =====
LANGSMITH_API_KEY=lsv2_pt_...your-key...
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=orbit-assignment-05

# ===== Airflow Configuration =====
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true

# ===== Application Configuration =====
LOG_LEVEL=INFO
PROJECT_NAME=Assignment_05
```

---

### 4ï¸âƒ£ Local Development Setup (Optional)

For local development without Docker:

```bash
# Create virtual environment
python -m venv .venv
# with uv in Linusx
uv venv .venv

# Activate virtual environment
# Windows (PowerShell):
.venv\Scripts\Activate.ps1
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
uv sync 

pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Start Services with Docker

```bash
# Build and start all services
cd docker
docker-compose build --no-cache
docker-compose up -d

# Wait 30-60 seconds for initialization
```

---

### 6ï¸âƒ£ Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow UI** | http://98.95.70.0:8080 | `airflow` / `airflow` |
| **MCP Server** | http://98.95.70.0:9000 | N/A (API) |
| **FastAPI Docs** | http://98.95.70.0:8000/docs | N/A |
| **Streamlit Dashboard** | http://98.95.70.0:8501 | N/A 


## ğŸ“‚ Project Structure

```
Assignment_05/
â”œâ”€â”€ ğŸ“ src/                         
â”‚   â”œâ”€â”€ ğŸ“ mcp_server/                   
â”‚   â”‚   â””â”€â”€ mcp_enrichment_client.py          
â”‚   â”œâ”€â”€ ğŸ“ dags/
â”‚   â”‚   â”œâ”€â”€ discover_dag.py                     
â”‚   â”‚   â”œâ”€â”€ process_pages_dag.py     
â”‚   â”‚   â”œâ”€â”€ ingest_dag.py            
â”‚   â”‚   â”œâ”€â”€ extraction_dag.py
â”‚   â”‚   â”œâ”€â”€ storing_dag.py
â”‚   â”‚   â”œâ”€â”€ dashboard_generation.py 
â”‚   â”‚   â”œâ”€â”€ eval_runner_dag.py
â”‚   â”‚   â”œâ”€â”€ master_orchestrator_dag.py        
â”‚   â”‚   â”œâ”€â”€ agentic_rag_dag.py      
â”‚   â”‚   â”œâ”€â”€ payload_agent_dag.py     
â”‚   â”‚   â””â”€â”€ enrichment_dag.py
â”‚   â”œâ”€â”€ ğŸ“ rag/                      
â”‚   â”‚   â”œâ”€â”€ ingest_to_pinecone.py    
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py          
â”‚   â”‚   â””â”€â”€ structured_extraction_search.py
â”‚   â”‚   â””â”€â”€ rag_models.py                
â”‚   â”œâ”€â”€ ğŸ“ payload_agent/           
â”‚   â”‚   â”œâ”€â”€ payload_agent.py         
â”‚   â”‚   â”œâ”€â”€ payload_workflow.py
â”‚   â”‚   â”œâ”€â”€ tools/rag_adapter.py 
â”‚   â”‚   â”œâ”€â”€ tools/validation.py
â”‚   â”‚   â””â”€â”€ tools/retrieval.py     
â”‚   â”œâ”€â”€ ğŸ“ tavily_agent/            
â”‚   â”‚   â”œâ”€â”€ main.py                  
â”‚   â”‚   â””â”€â”€ file_io_manager.py       
â”‚   â”œâ”€â”€ ğŸ“ discover/                 
â”‚   â”‚   â”œâ”€â”€ discover.py         
â”‚   â”‚   â””â”€â”€ process_discovered_pages.py       
â”‚   â”œâ”€â”€ ğŸ“ backend/                  
â”‚   â”‚   â””â”€â”€ rag_search_api.py                   
â”‚   â”œâ”€â”€ ğŸ“ frontend/ 
â”‚   â”‚   â”œâ”€â”€ eval_dashboard.py                
â”‚   â”‚   â””â”€â”€ streamlit_app.py         
â”‚   â”œâ”€â”€ ğŸ“ evals/                    
â”‚   â”‚   â””â”€â”€ results_evaluator.py             
â”‚   â””â”€â”€ ğŸ“ prompts/                 
â”‚       â”œâ”€â”€ pe_dashboard.md           
â”œâ”€â”€ ğŸ“ docker/                      
â”‚   â”œâ”€â”€ Dockerfile                   
â”‚   â”œâ”€â”€ docker-compose.yml           
â”‚   â”œâ”€â”€ .env.example                                
â”œâ”€â”€ ğŸ“ docs/                      
â”‚   â”œâ”€â”€ 00_START_HERE.md           
â”‚   â”œâ”€â”€ QUICKSTART.md                
â”‚   â”œâ”€â”€ EVALUATION_GUIDE.md         
â”‚   â”œâ”€â”€ REACT_QUICK_REFERENCE.md     
â”‚   â””â”€â”€ WORKFLOW_GRAPH.md           
â”œâ”€â”€ ğŸ“ tests/                       
â”‚   â”œâ”€â”€ test_payload_tools.py       
â”‚   â””â”€â”€ test_payload_workflow.py     
â”œâ”€â”€ .env.example                  
â”œâ”€â”€ pyproject.toml                  
â”œâ”€â”€ requirements.txt               
â”œâ”€â”€ Assignment5.md                 
â”œâ”€â”€ README.md                       


```

---

## Links

ğŸ“š [Full Technical Codelabs](https://codelabs-preview.appspot.com/?file_id=1hCRRMtxdtcyp1OVLlNYxGbYM1qvOBnoxT4442yt5ZXY#0) â€” Detailed walkthrough  
ğŸ“‹ [Assignment Requirements](./Assignment5.md) â€” Lab breakdown  
ğŸ¥ [Demo Video]() â€” Project walkthrough

---